{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import gc\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "Image.MAX_IMAGE_PIXELS = 10000000000  # Ignore PIL warnings about large images\n",
    "from tqdm import tqdm\n",
    "from typing import List, Tuple\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from torch.optim import AdamW\n",
    "import random\n",
    "import torch\n",
    "from torch import nn\n",
    "import cv2\n",
    "from torch import Tensor\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import traceback\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from time import time\n",
    "import wandb\n",
    "from albumentations import ImageOnlyTransform\n",
    "\n",
    "import segmentation_models_pytorch as smp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    seed = 1337\n",
    "    comp_name = 'vesuvius'\n",
    "    mode = \"train\"  # 'test'  # \"train\"\n",
    "\n",
    "    # ============== model cfg =============\n",
    "    model_name = 'Unet'\n",
    "    backbone = 'efficientnet-b0'  # 'se_resnext50_32x4d'\n",
    "    model_to_load = None  # '../model_checkpoints/vesuvius_notebook_clone_exp_holdout_3/models/Unet-zdim_6-epochs_30-step_15000-validId_3-epoch_9-dice_0.5195_dict.pt'\n",
    "    target_size = 1\n",
    "    in_chans = 4  # 8  # 6\n",
    "    pretrained = True\n",
    "    inf_weight = 'best'\n",
    "\n",
    "    # ============== training cfg =============\n",
    "    epochs = 50  # 15 # 30\n",
    "    train_steps = 15000\n",
    "    size = 224  # Size to shrink image to\n",
    "    tile_size = 224\n",
    "    stride = tile_size // 2\n",
    "\n",
    "    train_batch_size = 1\n",
    "    valid_batch_size = train_batch_size * 2\n",
    "    valid_id = 4\n",
    "    use_amp = True\n",
    "\n",
    "    scheduler = 'GradualWarmupSchedulerV2'  # 'CosineAnnealingLR'\n",
    "    min_lr = 1e-6\n",
    "    weight_decay = 1e-6\n",
    "    max_grad_norm = 1000\n",
    "    num_workers = 0\n",
    "\n",
    "    # objective_cv = 'binary'  # 'binary', 'multiclass', 'regression'\n",
    "    metric_direction = 'maximize'  # maximize, 'minimize'\n",
    "    # metrics = 'dice_coef'\n",
    "\n",
    "    # adamW warmup\n",
    "    warmup_factor = 10\n",
    "    lr = 1e-4 / warmup_factor\n",
    "\n",
    "    # ============== Experiment cfg =============\n",
    "    # ToDO consolidate these names into one\n",
    "    # exp_name = f'vesuvius_notebook_clone_exp_holdout_{valid_id}'\n",
    "    EXPERIMENT_NAME = f\"{model_name}-zdim_{in_chans}-epochs_{epochs}-validId_{valid_id}\"\n",
    "\n",
    "    # ============== Inference cfg =============\n",
    "    THRESHOLD = 0.3  # .52 score had a different value of .25\n",
    "\n",
    "    # ============== set dataset paths =============\n",
    "    comp_dir_path = '../'\n",
    "    comp_dataset_path = comp_dir_path + 'data/'\n",
    "    outputs_path = comp_dir_path + f'model_checkpoints/{EXPERIMENT_NAME}/'\n",
    "\n",
    "    submission_dir = outputs_path + 'submissions/'\n",
    "    submission_path = submission_dir + f'submission_{EXPERIMENT_NAME}.csv'\n",
    "    model_dir = outputs_path + 'models/'\n",
    "    figures_dir = outputs_path + 'figures/'\n",
    "\n",
    "    # ============== Augmentation =============\n",
    "    train_aug_list = [\n",
    "        # A.RandomResizedCrop(\n",
    "        #     size, size, scale=(0.85, 1.0)),\n",
    "        A.Resize(size, size),\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.VerticalFlip(p=0.5),\n",
    "        A.RandomBrightnessContrast(p=0.75),\n",
    "        A.ShiftScaleRotate(p=0.75),\n",
    "        A.OneOf([\n",
    "            A.GaussNoise(var_limit=[10, 50]),\n",
    "            A.GaussianBlur(),\n",
    "            A.MotionBlur(),\n",
    "        ], p=0.4),\n",
    "        A.GridDistortion(num_steps=5, distort_limit=0.3, p=0.5),\n",
    "        A.CoarseDropout(max_holes=1, max_width=int(size * 0.3), max_height=int(size * 0.3),\n",
    "                        mask_fill_value=0, p=0.5),\n",
    "        # A.Cutout(max_h_size=int(size * 0.6),\n",
    "        #          max_w_size=int(size * 0.6), num_holes=1, p=1.0),\n",
    "        A.Normalize(\n",
    "            mean=[0] * in_chans,\n",
    "            std=[1] * in_chans\n",
    "        ),\n",
    "        ToTensorV2(transpose_mask=True),\n",
    "    ]\n",
    "\n",
    "    valid_aug_list = [\n",
    "        A.Resize(size, size),\n",
    "        A.Normalize(\n",
    "            mean=[0] * in_chans,\n",
    "            std=[1] * in_chans\n",
    "        ),\n",
    "        ToTensorV2(transpose_mask=True),\n",
    "    ]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Setup Data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def read_image_and_labels(fragment_id: str, is_train: bool = True, mode: str = \"train\"):\n",
    "    images = []\n",
    "\n",
    "    mid = 65 // 2\n",
    "    start = mid - CFG.in_chans // 2\n",
    "    end = mid + CFG.in_chans // 2\n",
    "    idxs = range(start, end)\n",
    "\n",
    "    for i in tqdm(idxs):\n",
    "        image = cv2.imread(CFG.comp_dataset_path + f\"{mode}/{fragment_id}/surface_volume/{i:02}.tif\", 0)\n",
    "\n",
    "        pad0 = (CFG.tile_size - image.shape[0] % CFG.tile_size)\n",
    "        pad1 = (CFG.tile_size - image.shape[1] % CFG.tile_size)\n",
    "\n",
    "        image = np.pad(image, [(0, pad0), (0, pad1)], constant_values=0)\n",
    "\n",
    "        images.append(image)\n",
    "    images = np.stack(images, axis=2)  # Shape: (8288, 6496, 6)\n",
    "\n",
    "    print(f\"Length of image stack: {images.size}\")\n",
    "    if is_train:\n",
    "        labels = cv2.imread(CFG.comp_dataset_path + f\"train/{fragment_id}/inklabels.png\", 0)\n",
    "        labels = np.pad(labels, [(0, pad0), (0, pad1)], constant_values=0)\n",
    "\n",
    "        labels = labels.astype('float32')\n",
    "        labels /= 255.0  # Normalizing?\n",
    "    else:\n",
    "        labels = None\n",
    "\n",
    "    return images, labels"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def get_train_valid_dataset():\n",
    "    train_images = []\n",
    "    train_labels = []\n",
    "\n",
    "    valid_images = []\n",
    "    valid_labels = []\n",
    "    valid_xyxys = []\n",
    "\n",
    "    for frag_id in range(3, 5):\n",
    "        print(f\"Load images for fragment: {frag_id}\")\n",
    "        image, label = read_image_and_labels(frag_id)\n",
    "\n",
    "        x1_list = list(range(0, image.shape[1]-CFG.tile_size+1, CFG.stride))\n",
    "        y1_list = list(range(0, image.shape[0]-CFG.tile_size+1, CFG.stride))\n",
    "\n",
    "        for y1 in y1_list:\n",
    "            y2 = y1 + CFG.tile_size\n",
    "            for x1 in x1_list:\n",
    "                x2 = x1 + CFG.tile_size\n",
    "\n",
    "                if frag_id == CFG.valid_id:\n",
    "                    valid_images.append(image[y1:y2, x1:x2])\n",
    "                    valid_labels.append(label[y1:y2, x1:x2, None])\n",
    "\n",
    "                    valid_xyxys.append([x1, y1, x2, y2])\n",
    "                else:\n",
    "                    train_images.append(image[y1:y2, x1:x2])\n",
    "                    train_labels.append(label[y1:y2, x1:x2, None])\n",
    "\n",
    "    return train_images, train_labels, valid_images, valid_labels, valid_xyxys\n",
    "\n",
    "def get_transforms(data, cfg):\n",
    "    return A.Compose(cfg.train_aug_list) if data == 'train' else A.Compose(cfg.valid_aug_list)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create data object"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "Load images for fragment: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:03<00:00,  1.08it/s]\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of image stack: 163774464\n",
      "Load images for fragment: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:03<00:00,  1.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of image stack: 293429248\n"
     ]
    }
   ],
   "source": [
    "from Scripts.segmentation_model import ImageDataset\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(DEVICE)\n",
    "train_images, train_masks, valid_images, valid_masks, valid_xyxys = get_train_valid_dataset()\n",
    "valid_xyxys = np.stack(valid_xyxys)\n",
    "\n",
    "train_dataset = ImageDataset(train_images, labels=train_masks, transform=get_transforms(data='train', cfg=CFG))\n",
    "train_loader = DataLoader(train_dataset,\n",
    "                          batch_size=CFG.train_batch_size,\n",
    "                          shuffle=True,\n",
    "                          num_workers=CFG.num_workers, pin_memory=True, drop_last=True,\n",
    "                          )\n",
    "for images, labels in train_loader:\n",
    "    images = images.to(DEVICE)\n",
    "    labels = labels.to(DEVICE)\n",
    "    break"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 4, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "print(images[:, :3, :, :].shape)\n",
    "print(images.shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Define Your Model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, height, width):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "        self.pe = nn.Parameter(torch.zeros(1, d_model, height, width))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, :, :x.size(2), :x.size(3)]\n",
    "        return x\n",
    "\n",
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, dim_feedforward, dropout):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model, num_heads, dim_feedforward, dropout)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=6)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(2, 0, 1, 3)  # Reshape input tensor [batch_size, channels, height, width] to [height, batch_size, channels, width]\n",
    "        x = x.flatten(2)  # Flatten the height and channel dimensions\n",
    "        x = x.permute(1, 0, 2)  # Reshape back to [batch_size, sequence_length, channels]\n",
    "        x = self.transformer_encoder(x)\n",
    "        x = x.permute(1, 0, 2)  # Reshape back to [batch_size, sequence_length, channels]\n",
    "        return x\n",
    "\n",
    "\n",
    "class FCTransformer(nn.Module):\n",
    "    def __init__(self, d_model=301056, nhead=8, num_encoder_layers=6, dim_feedforward=2048, dropout=0.1, num_classes=2):\n",
    "        super(FCTransformer, self).__init__()\n",
    "        self.embedding_dim = d_model\n",
    "        self.pos_encoder = PositionalEncoding(d_model=d_model, height=224, width=224)\n",
    "        self.transformer_encoder = TransformerEncoder(d_model, nhead, dim_feedforward, dropout)\n",
    "        self.fc = nn.Linear(d_model, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pos_encoder(x)\n",
    "        x = self.transformer_encoder(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class FCTSegmentationModel(nn.Module):\n",
    "    def __init__(self, in_channels, num_classes):\n",
    "        super(FCTSegmentationModel, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        # self.transformer = TransformerEncoder(d_model=64, num_heads=8, dim_feedforward=256, dropout=0.1)\n",
    "        self.transformer = FCTransformer()\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, num_classes, kernel_size=1, stride=1),\n",
    "        )\n",
    "        self.positional_encoding = PositionalEncoding(d_model=64, height=224, width=224)\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(\"before encoder\")\n",
    "        print(x.shape)\n",
    "        x = self.encoder(x)\n",
    "        print(\"before positional encoder\")\n",
    "        print(x.shape)\n",
    "        x = self.positional_encoding(x)\n",
    "        print(\"before transformer\")\n",
    "        print(x.shape)\n",
    "        x = self.transformer(x)\n",
    "        print(\"before decoder\")\n",
    "        print(x.shape)\n",
    "        x = self.decoder(x)\n",
    "        print(\"Returning\")\n",
    "        print(x.shape)\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Begin your testing"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\c10\\core\\impl\\alloc_cpu.cpp:72] data. DefaultCPUAllocator: not enough memory: you tried to allocate 60423143424 bytes.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[55], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m model \u001B[38;5;241m=\u001B[39m \u001B[43mFCTSegmentationModel\u001B[49m\u001B[43m(\u001B[49m\u001B[43min_channels\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m4\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_classes\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;66;03m# inputs = torch.randn(1, 3, 256, 256)  # Example input tensor\u001B[39;00m\n\u001B[0;32m      3\u001B[0m model\u001B[38;5;241m.\u001B[39mto(DEVICE)\n",
      "Cell \u001B[1;32mIn[53], line 53\u001B[0m, in \u001B[0;36mFCTSegmentationModel.__init__\u001B[1;34m(self, in_channels, num_classes)\u001B[0m\n\u001B[0;32m     46\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mencoder \u001B[38;5;241m=\u001B[39m nn\u001B[38;5;241m.\u001B[39mSequential(\n\u001B[0;32m     47\u001B[0m     nn\u001B[38;5;241m.\u001B[39mConv2d(in_channels, \u001B[38;5;241m64\u001B[39m, kernel_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m3\u001B[39m, stride\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m, padding\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m),\n\u001B[0;32m     48\u001B[0m     nn\u001B[38;5;241m.\u001B[39mReLU(inplace\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m),\n\u001B[0;32m     49\u001B[0m     nn\u001B[38;5;241m.\u001B[39mConv2d(\u001B[38;5;241m64\u001B[39m, \u001B[38;5;241m64\u001B[39m, kernel_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m3\u001B[39m, stride\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m, padding\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m),\n\u001B[0;32m     50\u001B[0m     nn\u001B[38;5;241m.\u001B[39mReLU(inplace\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m),\n\u001B[0;32m     51\u001B[0m )\n\u001B[0;32m     52\u001B[0m \u001B[38;5;66;03m# self.transformer = TransformerEncoder(d_model=64, num_heads=8, dim_feedforward=256, dropout=0.1)\u001B[39;00m\n\u001B[1;32m---> 53\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtransformer \u001B[38;5;241m=\u001B[39m \u001B[43mFCTransformer\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     54\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdecoder \u001B[38;5;241m=\u001B[39m nn\u001B[38;5;241m.\u001B[39mSequential(\n\u001B[0;32m     55\u001B[0m     nn\u001B[38;5;241m.\u001B[39mConv2d(\u001B[38;5;241m64\u001B[39m, \u001B[38;5;241m64\u001B[39m, kernel_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m3\u001B[39m, stride\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m, padding\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m),\n\u001B[0;32m     56\u001B[0m     nn\u001B[38;5;241m.\u001B[39mReLU(inplace\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m),\n\u001B[0;32m     57\u001B[0m     nn\u001B[38;5;241m.\u001B[39mConv2d(\u001B[38;5;241m64\u001B[39m, num_classes, kernel_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m, stride\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m),\n\u001B[0;32m     58\u001B[0m )\n\u001B[0;32m     59\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpositional_encoding \u001B[38;5;241m=\u001B[39m PositionalEncoding(d_model\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m64\u001B[39m, height\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m224\u001B[39m, width\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m224\u001B[39m)\n",
      "Cell \u001B[1;32mIn[53], line 32\u001B[0m, in \u001B[0;36mFCTransformer.__init__\u001B[1;34m(self, d_model, nhead, num_encoder_layers, dim_feedforward, dropout, num_classes)\u001B[0m\n\u001B[0;32m     30\u001B[0m \u001B[38;5;28msuper\u001B[39m(FCTransformer, \u001B[38;5;28mself\u001B[39m)\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__init__\u001B[39m()\n\u001B[0;32m     31\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39membedding_dim \u001B[38;5;241m=\u001B[39m d_model\n\u001B[1;32m---> 32\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpos_encoder \u001B[38;5;241m=\u001B[39m \u001B[43mPositionalEncoding\u001B[49m\u001B[43m(\u001B[49m\u001B[43md_model\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43md_model\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mheight\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m224\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mwidth\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m224\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m     33\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtransformer_encoder \u001B[38;5;241m=\u001B[39m TransformerEncoder(d_model, nhead, dim_feedforward, dropout)\n\u001B[0;32m     34\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfc \u001B[38;5;241m=\u001B[39m nn\u001B[38;5;241m.\u001B[39mLinear(d_model, num_classes)\n",
      "Cell \u001B[1;32mIn[53], line 6\u001B[0m, in \u001B[0;36mPositionalEncoding.__init__\u001B[1;34m(self, d_model, height, width)\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mheight \u001B[38;5;241m=\u001B[39m height\n\u001B[0;32m      5\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mwidth \u001B[38;5;241m=\u001B[39m width\n\u001B[1;32m----> 6\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpe \u001B[38;5;241m=\u001B[39m nn\u001B[38;5;241m.\u001B[39mParameter(\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mzeros\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43md_model\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mheight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mwidth\u001B[49m\u001B[43m)\u001B[49m)\n",
      "\u001B[1;31mRuntimeError\u001B[0m: [enforce fail at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\c10\\core\\impl\\alloc_cpu.cpp:72] data. DefaultCPUAllocator: not enough memory: you tried to allocate 60423143424 bytes."
     ]
    }
   ],
   "source": [
    "model = FCTSegmentationModel(in_channels=4, num_classes=2)\n",
    "# inputs = torch.randn(1, 3, 256, 256)  # Example input tensor\n",
    "model.to(DEVICE)\n",
    "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Number of model params is: {num_params:,}\")\n",
    "outputs = model(images)  # [:, :3, :, :]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Input type (torch.cuda.FloatTensor) and weight type (torch.FloatTensor) should be the same",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[16], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m patch_embed \u001B[38;5;241m=\u001B[39m nn\u001B[38;5;241m.\u001B[39mConv2d(\u001B[38;5;241m4\u001B[39m, \u001B[38;5;241m256\u001B[39m, kernel_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m16\u001B[39m, stride\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m16\u001B[39m)\n\u001B[1;32m----> 2\u001B[0m \u001B[43mpatch_embed\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimages\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\.conda\\envs\\vesuvius\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32m~\\.conda\\envs\\vesuvius\\lib\\site-packages\\torch\\nn\\modules\\conv.py:463\u001B[0m, in \u001B[0;36mConv2d.forward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    462\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[1;32m--> 463\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_conv_forward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\.conda\\envs\\vesuvius\\lib\\site-packages\\torch\\nn\\modules\\conv.py:459\u001B[0m, in \u001B[0;36mConv2d._conv_forward\u001B[1;34m(self, input, weight, bias)\u001B[0m\n\u001B[0;32m    455\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpadding_mode \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mzeros\u001B[39m\u001B[38;5;124m'\u001B[39m:\n\u001B[0;32m    456\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m F\u001B[38;5;241m.\u001B[39mconv2d(F\u001B[38;5;241m.\u001B[39mpad(\u001B[38;5;28minput\u001B[39m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reversed_padding_repeated_twice, mode\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpadding_mode),\n\u001B[0;32m    457\u001B[0m                     weight, bias, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstride,\n\u001B[0;32m    458\u001B[0m                     _pair(\u001B[38;5;241m0\u001B[39m), \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdilation, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgroups)\n\u001B[1;32m--> 459\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconv2d\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbias\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstride\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    460\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpadding\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdilation\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgroups\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mRuntimeError\u001B[0m: Input type (torch.cuda.FloatTensor) and weight type (torch.FloatTensor) should be the same"
     ]
    }
   ],
   "source": [
    "patch_embed = nn.Conv2d(4, 256, kernel_size=16, stride=16)\n",
    "patch_embed(images)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([1, 8, 28, 28])"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv = nn.Conv2d(4, 8, kernel_size=8, stride=8, padding=0).to(DEVICE)\n",
    "r = conv(images)\n",
    "r.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "voxel = torch.randn(1, 8, 224, 224)\n",
    "vox_conv = nn.Conv2d(in_channels=8, out_channels=1, kernel_size=8, stride=8, padding=0)\n",
    "vox_conv(voxel).shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "voxel = torch.randn(1, 8, 224, 224)\n",
    "vox_conv = nn.Conv2d(in_channels=8, out_channels=1, kernel_size=8, stride=8, padding=0)\n",
    "vox_conv(voxel).shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "---"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of model params is: 4,748,289\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "class FCTBlock(nn.Module):\n",
    "    def __init__(self, d_model, nhead, dim_feedforward, dropout=0.1):\n",
    "        super(FCTBlock, self).__init__()\n",
    "        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n",
    "        self.linear1 = nn.Linear(d_model, dim_feedforward)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(dim_feedforward, d_model)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        attn_output, _ = self.self_attn(x, x, x)\n",
    "        x2 = x + self.dropout1(attn_output)\n",
    "        x = self.norm1(x2)\n",
    "        x2 = self.linear2(self.dropout(F.relu(self.linear1(x))))\n",
    "        x = x + self.dropout2(x2)\n",
    "        x = self.norm2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class FCTSegmentationModel(nn.Module):\n",
    "    def __init__(self, in_channels, d_model, nhead, num_layers, dim_feedforward, dropout, num_classes):\n",
    "        super(FCTSegmentationModel, self).__init__()\n",
    "        self.embedding_dim = d_model\n",
    "        self.conv = nn.Conv2d(in_channels, d_model, kernel_size=3, stride=1, padding=1)\n",
    "        self.pos_encoder = PositionalEncoding(d_model)\n",
    "        self.transformer = nn.ModuleList([\n",
    "            FCTBlock(d_model, nhead, dim_feedforward, dropout=dropout) for _ in range(num_layers)\n",
    "        ])\n",
    "        self.fc = nn.Conv2d(d_model, num_classes, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = x.flatten(2).transpose(1, 2)\n",
    "        x = self.pos_encoder(x)\n",
    "        for transformer_block in self.transformer:\n",
    "            x = transformer_block(x)\n",
    "        x = x.transpose(1, 2).reshape(x.shape[0], self.embedding_dim, x.shape[3], x.shape[2])\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Usage example\n",
    "model = FCTSegmentationModel(in_channels=4, d_model=256, nhead=4, num_layers=6, dim_feedforward=1024, dropout=0.1, num_classes=1).to(DEVICE)\n",
    "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Number of model params is: {num_params:,}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 196.00 MiB (GPU 0; 6.00 GiB total capacity; 5.14 GiB already allocated; 0 bytes free; 5.15 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mOutOfMemoryError\u001B[0m                          Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[9], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimages\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\.conda\\envs\\vesuvius\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "Cell \u001B[1;32mIn[8], line 63\u001B[0m, in \u001B[0;36mFCTSegmentationModel.forward\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m     61\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpos_encoder(x)\n\u001B[0;32m     62\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m transformer_block \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtransformer:\n\u001B[1;32m---> 63\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[43mtransformer_block\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     64\u001B[0m x \u001B[38;5;241m=\u001B[39m x\u001B[38;5;241m.\u001B[39mtranspose(\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m2\u001B[39m)\u001B[38;5;241m.\u001B[39mreshape(x\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m], \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39membedding_dim, x\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m3\u001B[39m], x\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m2\u001B[39m])\n\u001B[0;32m     65\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfc(x)\n",
      "File \u001B[1;32m~\\.conda\\envs\\vesuvius\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "Cell \u001B[1;32mIn[8], line 41\u001B[0m, in \u001B[0;36mFCTBlock.forward\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m     39\u001B[0m x2 \u001B[38;5;241m=\u001B[39m x \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdropout1(attn_output)\n\u001B[0;32m     40\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnorm1(x2)\n\u001B[1;32m---> 41\u001B[0m x2 \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlinear2(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdropout(\u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrelu\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlinear1\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m))\n\u001B[0;32m     42\u001B[0m x \u001B[38;5;241m=\u001B[39m x \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdropout2(x2)\n\u001B[0;32m     43\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnorm2(x)\n",
      "File \u001B[1;32m~\\.conda\\envs\\vesuvius\\lib\\site-packages\\torch\\nn\\functional.py:1457\u001B[0m, in \u001B[0;36mrelu\u001B[1;34m(input, inplace)\u001B[0m\n\u001B[0;32m   1455\u001B[0m     result \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mrelu_(\u001B[38;5;28minput\u001B[39m)\n\u001B[0;32m   1456\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1457\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrelu\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1458\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m result\n",
      "\u001B[1;31mOutOfMemoryError\u001B[0m: CUDA out of memory. Tried to allocate 196.00 MiB (GPU 0; 6.00 GiB total capacity; 5.14 GiB already allocated; 0 bytes free; 5.15 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "model(images)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
