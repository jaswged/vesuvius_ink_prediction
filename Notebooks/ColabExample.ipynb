{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting einops\n",
      "  Downloading einops-0.6.0-py3-none-any.whl (41 kB)\n",
      "Installing collected packages: einops\n",
      "Successfully installed einops-0.6.0\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torch in c:\\programdata\\anaconda3\\lib\\site-packages (1.12.1)\n",
      "Collecting torch\n",
      "  Downloading torch-2.0.0-cp39-cp39-win_amd64.whl (172.3 MB)\n",
      "Requirement already satisfied: torchvision in c:\\programdata\\anaconda3\\lib\\site-packages (0.13.1)\n",
      "Collecting torchvision\n",
      "  Downloading torchvision-0.15.1-cp39-cp39-win_amd64.whl (1.2 MB)\n",
      "Requirement already satisfied: torchaudio in c:\\programdata\\anaconda3\\lib\\site-packages (0.12.1)\n",
      "Collecting torchaudio\n",
      "  Downloading torchaudio-2.0.1-cp39-cp39-win_amd64.whl (2.1 MB)\n",
      "Requirement already satisfied: sympy in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (1.10.1)\n",
      "Requirement already satisfied: networkx in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (2.7.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (4.1.1)\n",
      "Requirement already satisfied: jinja2 in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (2.11.3)\n",
      "Requirement already satisfied: filelock in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (3.6.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from torchvision) (9.0.1)\n",
      "Requirement already satisfied: numpy in c:\\programdata\\anaconda3\\lib\\site-packages (from torchvision) (1.21.5)\n",
      "Requirement already satisfied: requests in c:\\programdata\\anaconda3\\lib\\site-packages (from torchvision) (2.27.1)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\programdata\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.0.1)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->torchvision) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->torchvision) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->torchvision) (1.26.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->torchvision) (3.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\programdata\\anaconda3\\lib\\site-packages (from sympy->torch) (1.2.1)\n",
      "Installing collected packages: torch, torchvision, torchaudio\n",
      "Successfully installed torch-2.0.0 torchaudio-2.0.1 torchvision-0.15.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The scripts convert-caffe2-to-onnx.exe, convert-onnx-to-caffe2.exe and torchrun.exe are installed in 'C:\\Users\\gamer\\AppData\\Roaming\\Python\\Python39\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting wandb\n",
      "  Downloading wandb-0.14.0-py3-none-any.whl (2.0 MB)\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from wandb) (8.0.4)\n",
      "Requirement already satisfied: setuptools in c:\\programdata\\anaconda3\\lib\\site-packages (from wandb) (61.2.0)\n",
      "Collecting setproctitle\n",
      "  Downloading setproctitle-1.3.2-cp39-cp39-win_amd64.whl (11 kB)\n",
      "Collecting GitPython!=3.1.29,>=1.0.0\n",
      "  Downloading GitPython-3.1.31-py3-none-any.whl (184 kB)\n",
      "Collecting docker-pycreds>=0.4.0\n",
      "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
      "Requirement already satisfied: PyYAML in c:\\programdata\\anaconda3\\lib\\site-packages (from wandb) (6.0)\n",
      "Collecting pathtools\n",
      "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
      "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from wandb) (3.19.1)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from wandb) (2.27.1)\n",
      "Requirement already satisfied: appdirs>=1.4.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from wandb) (1.4.4)\n",
      "Requirement already satisfied: typing-extensions in c:\\programdata\\anaconda3\\lib\\site-packages (from wandb) (4.1.1)\n",
      "Collecting sentry-sdk>=1.0.0\n",
      "  Downloading sentry_sdk-1.18.0-py2.py3-none-any.whl (194 kB)\n",
      "Requirement already satisfied: psutil>=5.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from wandb) (5.8.0)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from Click!=8.0.0,>=7.0->wandb) (0.4.4)\n",
      "Requirement already satisfied: six>=1.4.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
      "Collecting gitdb<5,>=4.0.1\n",
      "  Downloading gitdb-4.0.10-py3-none-any.whl (62 kB)\n",
      "Collecting smmap<6,>=3.0.1\n",
      "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (1.26.9)\n",
      "Collecting urllib3<1.27,>=1.21.1\n",
      "  Downloading urllib3-1.26.15-py2.py3-none-any.whl (140 kB)\n",
      "Building wheels for collected packages: pathtools\n",
      "  Building wheel for pathtools (setup.py): started\n",
      "  Building wheel for pathtools (setup.py): finished with status 'done'\n",
      "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8806 sha256=94d43af511ed770dde142f14b27a4d973083bbd6caab80bde03f0dcadcc8b1f3\n",
      "  Stored in directory: c:\\users\\gamer\\appdata\\local\\pip\\cache\\wheels\\b7\\0a\\67\\ada2a22079218c75a88361c0782855cc72aebc4d18d0289d05\n",
      "Successfully built pathtools\n",
      "Installing collected packages: smmap, urllib3, gitdb, setproctitle, sentry-sdk, pathtools, GitPython, docker-pycreds, wandb\n",
      "Successfully installed GitPython-3.1.31 docker-pycreds-0.4.0 gitdb-4.0.10 pathtools-0.1.2 sentry-sdk-1.18.0 setproctitle-1.3.2 smmap-5.0.0 urllib3-1.26.15 wandb-0.14.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The scripts wandb.exe and wb.exe are installed in 'C:\\Users\\gamer\\AppData\\Roaming\\Python\\Python39\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "conda-repo-cli 1.0.4 requires pathlib, which is not installed.\n"
     ]
    }
   ],
   "source": [
    "!pip install einops\n",
    "!pip install --upgrade torch torchvision torchaudio\n",
    "!pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1668384032.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;36m  Input \u001B[1;32mIn [2]\u001B[1;36m\u001B[0m\n\u001B[1;33m    os.environ['KAGGLE_USERNAME'] = ##\u001B[0m\n\u001B[1;37m                                    ^\u001B[0m\n\u001B[1;31mSyntaxError\u001B[0m\u001B[1;31m:\u001B[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['KAGGLE_USERNAME'] = ##\n",
    "os.environ['KAGGLE_KEY'] = ##"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!kaggle competitions download -c vesuvius-challenge-ink-detection\n",
    "!unzip /content/vesuvius-challenge-ink-detection.zip"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import wandb\n",
    "!wandb login"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Pretraining a Masked Auto-Encoder\n",
    "\n",
    "Slices of tomography have some kind of structure that isn't obvious to us. By pre-training an autoencoder to reconstruct these slices, we obtain a feature representation that captures some of this internal structure, and can be leveraged for downstream tasks. I theorize that even the outer layers, not useful for ink detection, may still be useful enough for unsupervised pretraining. For downstream supervised learning, we use this pretrained MAE to extract the \"features\" for each slice, and can then train another model on top of these representations."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import PIL.Image as Image\n",
    "import torch.utils.data as data\n",
    "from typing import List, Tuple\n",
    "from pathlib import Path\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from einops import rearrange, repeat\n",
    "from tqdm.auto import tqdm"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class Patch3DDataset(torch.utils.data.IterableDataset):\n",
    "    def __init__(\n",
    "            self,\n",
    "            fragments: List[Path],\n",
    "            patch_shape: Tuple[int, int, int],\n",
    "            buffer_size: int = 50000\n",
    "    ):\n",
    "        self.fragments = sorted(map(lambda path: path.resolve(), fragments))\n",
    "        self.z_dim, self.y_dim, self.x_dim = patch_shape\n",
    "        # self.load_inklabels = load_inklabels\n",
    "        # self.filter_edge_pixels = filter_edge_pixels\n",
    "\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Lambda(lambda patch: patch / 65535.0),\n",
    "            transforms.Lambda(lambda patch: torch.tensor(patch, dtype=torch.float32)),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomVerticalFlip()\n",
    "        ])\n",
    "\n",
    "        self.buffer_size = buffer_size\n",
    "\n",
    "    def __iter__(self):\n",
    "        # buffer used for pseudo-shuffing so the patches aren't returned completely sequentially\n",
    "        buffer = []\n",
    "\n",
    "        def add_to_buffer(item):\n",
    "            if len(buffer) < self.buffer_size:\n",
    "                buffer.append(item)\n",
    "            else:\n",
    "                idx = random.randint(0, self.buffer_size - 1)\n",
    "                buffer[idx] = item\n",
    "\n",
    "        def yield_random_from_buffer():\n",
    "            idx = random.randint(0, len(buffer) - 1)\n",
    "            return buffer.pop(idx)\n",
    "\n",
    "        # Load sequentially\n",
    "        while True: # infinite \"streaming\" dataset, use the number of steps to trigger stopping\n",
    "            for fragment_id, fragment_path in enumerate(self.fragments):\n",
    "                fragment_path = fragment_path.resolve()  # absolute path\n",
    "                mask = np.array(Image.open(str(fragment_path / \"mask.png\")).convert(\"1\"))\n",
    "                fragment_y, fragment_x = mask.shape\n",
    "                y_pad = self.y_dim - (fragment_y % self.y_dim)\n",
    "                x_pad = self.x_dim - (fragment_x % self.x_dim)\n",
    "                mask = np.pad(mask, pad_width = ((0, y_pad), (0, x_pad)), mode='constant', constant_values=0)\n",
    "\n",
    "                surface_volume_paths = sorted(\n",
    "                    (fragment_path / \"surface_volume\").rglob(\"*.tif\")\n",
    "                )\n",
    "\n",
    "                for z_idx in range(0, len(surface_volume_paths), self.z_dim):\n",
    "                    images = [\n",
    "                        np.array(Image.open(fn)) for fn in surface_volume_paths[z_idx:z_idx + self.z_dim]\n",
    "                    ]\n",
    "                    image_stack = np.stack(images, axis=0)\n",
    "\n",
    "                    image_stack = np.pad(image_stack, pad_width = ((0, 0), (0, y_pad), (0, x_pad)), mode='constant', constant_values=0)\n",
    "                    for y in range(0, mask.shape[0], self.y_dim):\n",
    "                        for x in range(0, mask.shape[1], self.x_dim):\n",
    "                            mask_chunk = mask[y:y+self.y_dim, x:x + self.x_dim]\n",
    "                            patch = image_stack[:, y:y+self.y_dim, x:x+self.x_dim]\n",
    "                            mask_mean = np.mean(mask_chunk)\n",
    "                            # only train on patches that are >= 30% data and a full patch\n",
    "                            if mask_mean < 0.3 or patch.shape[0] != self.z_dim:\n",
    "                                continue\n",
    "                            else:\n",
    "                                add_to_buffer((patch, mask_chunk))\n",
    "                                # if buffer full, then we are ready to randomly return a patch\n",
    "                                if len(buffer) == self.buffer_size:\n",
    "                                    random_patch, random_mask_chunk = yield_random_from_buffer()\n",
    "                                    yield self.transform(random_patch), random_mask_chunk"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, n_heads, embed_dim, d_qkv, dropout):\n",
    "        super().__init__()\n",
    "        self.d_qkv = d_qkv\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.to_qkv = nn.Linear(embed_dim, d_qkv * n_heads * 3, bias=False)\n",
    "        self.attn_dropout_p = dropout\n",
    "        self.out_proj = nn.Linear(d_qkv * n_heads, embed_dim, bias=False)\n",
    "        self.resid_dropout1 = nn.Dropout(dropout)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(embed_dim, 4 * embed_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(4 * embed_dim, embed_dim)\n",
    "        )\n",
    "        self.resid_dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, X):\n",
    "        normed1 = self.norm1(X)\n",
    "        Q, K, V = rearrange(self.to_qkv(normed1), \"b l (h ddd) -> b h l ddd\", ddd=(3 * self.d_qkv)).chunk(3, dim=-1) # b, h, l, d_attn\n",
    "        attn = F.scaled_dot_product_attention(Q, K, V, dropout_p=self.attn_dropout_p, is_causal=False).transpose(1, 2) # b, l, h, d_attn\n",
    "        attn_out = X + self.resid_dropout1(self.out_proj(attn.flatten(2, 3)))\n",
    "        normed2 = self.norm2(attn_out)\n",
    "        return attn_out + self.resid_dropout2(self.ffn(normed2))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class MAEVisionTransformer(nn.Module):\n",
    "    def __init__(self, img_size, patch_size, n_channels, mask_prob,\n",
    "                 encoder_depth, decoder_depth, n_heads, embed_dim, d_qkv, dropout=0.1):\n",
    "        super().__init__()\n",
    "        assert img_size % patch_size == 0, \"Image must divide evenly into mini-patches.\"\n",
    "        self.mask_prob = mask_prob\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.n_channels = n_channels\n",
    "        self.patch_emb = nn.Linear(patch_size**2 * n_channels, embed_dim)\n",
    "        self.pos_embs = nn.Parameter(torch.zeros(((img_size // patch_size)**2, embed_dim)) * 0.1)\n",
    "        self.mask_token = nn.Parameter(torch.randn((embed_dim,)) * 0.1)\n",
    "        self.encoder = nn.Sequential(*[\n",
    "            TransformerBlock(n_heads, embed_dim, d_qkv, dropout) for _ in range(encoder_depth)\n",
    "        ])\n",
    "        self.decoder = nn.Sequential(*[\n",
    "            TransformerBlock(n_heads, embed_dim, d_qkv, dropout) for _ in range(decoder_depth)\n",
    "        ])\n",
    "        self.output_head = nn.Sequential(\n",
    "            nn.Linear(embed_dim, 4 * embed_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(4 * embed_dim, patch_size**2 * n_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, X, training=True):\n",
    "        # Embed patches, apply positional embeddings\n",
    "        patches = rearrange(X, \"b c (h1 h2) (w1 w2) -> b (h1 w1) (c h2 w2)\", h2=self.patch_size, w2=self.patch_size)\n",
    "        seq = self.patch_emb(patches) + self.pos_embs.unsqueeze(0)\n",
    "        b, l, d = seq.shape\n",
    "\n",
    "        # If training, keep subsample of patches\n",
    "        if training:\n",
    "            with torch.no_grad():\n",
    "                n_to_keep = int(np.floor(l * (1 - self.mask_prob)))\n",
    "                perm = torch.randperm(l)\n",
    "                unmasked_idxs = perm[:n_to_keep]\n",
    "                masked_idxs = perm[n_to_keep:]\n",
    "                inv_perm = torch.argsort(perm)\n",
    "            seq = seq[:, perm[:n_to_keep], :]\n",
    "\n",
    "        # Apply encoder\n",
    "        for block in self.encoder:\n",
    "            seq = block(seq)\n",
    "\n",
    "        if not training:\n",
    "            return seq\n",
    "\n",
    "        # Add back masked patches, positional embeddings, unshuffle\n",
    "        mask_chunk = repeat(self.mask_token, \"d -> b l d\", b=seq.shape[0], l=l - n_to_keep)\n",
    "        pos_emb_chunk = self.pos_embs[perm[n_to_keep:], :].unsqueeze(0)\n",
    "        mask_chunk = mask_chunk + pos_emb_chunk\n",
    "        seq = torch.cat([seq, mask_chunk], dim=1)\n",
    "        seq = seq[:, inv_perm, :]\n",
    "\n",
    "        # Apply decoder\n",
    "        for block in self.decoder:\n",
    "            seq = block(seq)\n",
    "\n",
    "        # Output\n",
    "        output =  self.output_head(seq) # batch, n_patches, patch_size\n",
    "        return masked_idxs, output"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "vit = MAEVisionTransformer(64, 16, 5, 0.75, 16, 8, 12, 768, 64, dropout=0.1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "base_path = Path(\"/content/\")\n",
    "train_path = base_path / \"train\"\n",
    "all_fragments = sorted([f.name for f in train_path.iterdir()])\n",
    "print(\"All fragments:\", all_fragments)\n",
    "train_fragments = [train_path / fragment_name for fragment_name in all_fragments]\n",
    "train_dset = Patch3DDataset(fragments=train_fragments, patch_shape=(5, 64, 64))\n",
    "train_loader = torch.utils.data.DataLoader(train_dset, batch_size=128, shuffle=False, pin_memory=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "wandb.init(\n",
    "    project=\"scroll-transformer\",\n",
    "    config={\"lr\": 1e-4, \"wd\": 1e-3}\n",
    ")\n",
    "\n",
    "CKPT_SAVE_DIR = \"/content/drive/MyDrive/scroll_vit_checkpoints/\"\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "vit.to(device)\n",
    "criterion = nn.L1Loss()\n",
    "optimizer = torch.optim.AdamW(vit.parameters(), lr=1e-4, weight_decay=1e-3)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.99)\n",
    "\n",
    "max_steps = 25000\n",
    "steps = 0\n",
    "running_loss = 0.0\n",
    "print_every = 50\n",
    "while True:\n",
    "    for it, batch in enumerate(train_loader):\n",
    "        steps += 1\n",
    "        X = batch[0].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        masked_idxs, out_patches = vit(X)\n",
    "        preds = out_patches[:, masked_idxs, :]\n",
    "        original_patches = rearrange(X, \"b c (h1 h2) (w1 w2) -> b (h1 w1) (c h2 w2)\", h2=vit.patch_size, w2=vit.patch_size)\n",
    "        targets = original_patches[:, masked_idxs, :]\n",
    "        loss = criterion(preds, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        wandb.log({\"train_loss\": loss.item(), \"lr\": scheduler.get_last_lr()[0]})\n",
    "        if steps % print_every == 0:\n",
    "            print(f\"STEP {steps} | LOSS: {running_loss / print_every:.3f} | LR: {scheduler.get_last_lr()[0]:.6f}\")\n",
    "            running_loss = 0.0\n",
    "            scheduler.step()\n",
    "        if steps % 5000 == 0:\n",
    "            torch.save(vit, CKPT_SAVE_DIR + \"vit-\" + str(steps) + \".ckpt\")\n",
    "        if steps >= max_steps:\n",
    "            break\n",
    "    if steps >= max_steps:\n",
    "        break\n",
    "\n",
    "torch.save(vit, CKPT_SAVE_DIR + \"vit-final.ckpt\")"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
